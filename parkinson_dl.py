# -*- coding: utf-8 -*-
"""Parkinson_DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QchiGxzO1glsu023lppu34MFo-AvPSWb
"""

!pip install ucimlrepo

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import TensorDataset, DataLoader
import torch
import torch.optim
import torch.nn as nn
import numpy as np
import torchvision
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt

from ucimlrepo import fetch_ucirepo

# fetch dataset
parkinsons_telemonitoring = fetch_ucirepo(id=189)

# data (as pandas dataframes)
df_full=parkinsons_telemonitoring.data.original

X=df_full.drop(columns=['motor_UPDRS','total_UPDRS'])
y_motor=df_full['motor_UPDRS']
y_total=df_full['total_UPDRS']

"""Now we define the first model for the comparison: a simple MLP.
The task in this case is a point to point regression.
This model is a simple baseline, it will have mediocre performance
"""

#First, data preprocessing:
#drop columns we don't need and the targets
features_mlp=df_full.drop(columns=['subject#','motor_UPDRS','total_UPDRS'])
#and focus on one target, if regression on the other target is needed simply change this line
target_mlp=df_full['motor_UPDRS']

#Then divide the data in train an testing and use a Standard Scaler to scale it
X_train, X_test, y_train, y_test = train_test_split(features_mlp, target_mlp, test_size=0.2, random_state=42)
scaler=StandardScaler()
y_scaler=StandardScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)
y_train=y_scaler.fit_transform(y_train.values.reshape(-1,1))
y_test=y_scaler.transform(y_test.values.reshape(-1,1))

#convert data to tensor and create dataloaders
X_train=torch.tensor(X_train,dtype=torch.float32)
y_train=torch.tensor(y_train,dtype=torch.float32)

dataset=TensorDataset(X_train,y_train)
loader=DataLoader(dataset,batch_size=32,shuffle=True)

test_dataset=TensorDataset(torch.tensor(X_test,dtype=torch.float32),torch.tensor(y_test,dtype=torch.float32))
test_loader = DataLoader(test_dataset, batch_size=32)

#Then define the model
class MLP(torch.nn.Module):
    def __init__(self, input_dim):
        super(MLP, self).__init__()
        self.model = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 32),
            torch.nn.ReLU(),
            torch.nn.Linear(32, 1)
        )
        self.loss_fn = torch.nn.MSELoss()
        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)

    def forward(self, x):
        return self.model(x)


model=MLP(input_dim=X_train.shape[1])
loss_fn=nn.MSELoss()
optimizer=torch.optim.Adam(model.parameters(),lr=1e-4)

#Training has the same number of epochs for every model
for epoch in range(2000):
    total_loss=0
    for xb,yb in loader:
        output=model(xb)
        loss=model.loss_fn(output,yb)
        model.optimizer.zero_grad()
        loss.backward()
        model.optimizer.step()
        total_loss+=loss.item()*xb.size(0)

    if (epoch + 1) % 20 == 0:
        print(f"RNN Epoch {epoch+1}, Loss: {total_loss:.4f}")

#Then define the function for testing
def test_model(model, test_loader, loss_fn,scaler_y):
  model.eval()
  y_true_list=[]
  y_pred_list=[]

  with torch.no_grad():
    total_loss=0
    for xb,yb in test_loader:
      pred=model(xb)
      y_pred_list.append(pred.numpy())
      y_true_list.append(yb.numpy())

  y_pred = np.concatenate(y_pred_list, axis=0)
  y_true = np.concatenate(y_true_list, axis=0)

  #bring back data to an interpretable form
  y_pred_rescaled = scaler_y.inverse_transform(y_pred)
  y_true_rescaled = scaler_y.inverse_transform(y_true)

  mse = mean_squared_error(y_true_rescaled, y_pred_rescaled)
  mae = mean_absolute_error(y_true_rescaled, y_pred_rescaled)
  r2 = r2_score(y_true_rescaled, y_pred_rescaled)

  print(f"Test MSE: {mse:.2f}")
  print(f"Test MAE: {mae:.2f}")
  print(f"R² Score: {r2:.4f}")
  #First a simple line plot
  plt.figure(figsize=(12, 5))
  plt.plot(y_true_rescaled, label="Real values", color="blue")
  plt.plot(y_pred_rescaled, label="Predicted values", color="orange")
  plt.title("Predicted vs real values curve")
  plt.xlabel("Sample")
  plt.ylabel("UPDRS")
  plt.legend()
  plt.grid(True)
  plt.tight_layout()
  plt.show()

  #Scatter
  plt.figure(figsize=(6, 6))
  plt.scatter(y_true_rescaled, y_pred_rescaled, alpha=0.6, color="royalblue", edgecolors='k')
  plt.plot([y_true_rescaled.min(), y_true_rescaled.max()],
         [y_true_rescaled.min(), y_true_rescaled.max()],
         color='red', linestyle='--', label="Linea ideale")

  plt.title("Scatter plot: Real vs Predicted values")
  plt.xlabel("Real values (UPDRS)")
  plt.ylabel("Predicted values")
  plt.legend()
  plt.grid(True)
  plt.tight_layout()
  plt.show()

test_model(model, test_loader,loss_fn,y_scaler)

"""Now implementing the RNN model"""

#First data processing and organizzation
#group data by subject
df_grouped=df_full.groupby('subject#')

#sliding windows parameters
window_size=5
target_col="motor_UPDRS"

#empty lists where put data in sequences
X_seq=[]
y_seq=[]

#per ogni paziente
for subject_id, df_subject in df_grouped:
  group_sorted=df_subject.sort_values("test_time").reset_index(drop=True)

  features=group_sorted.drop(columns=['subject#','test_time','motor_UPDRS','total_UPDRS'])
  target=group_sorted[target_col]
  #for every i with at least five following entries
  for i in range(len(features)-window_size):
    X_seq.append(features[i:i+window_size]) #append that five following entries to the list as a single entry
    y_seq.append(target[i+window_size])

X_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)
#split the data and transform the splitted data in tensors
X_seq_train=np.array(X_seq)
y_seq_train=np.array(y_seq)
X_seq_test=np.array(X_seq)
y_seq_test=np.array(y_seq)

# Scale x
num_features = X_seq_train.shape[2]
X_flat_train = X_seq_train.reshape(-1, num_features)
scaler_X = StandardScaler().fit(X_flat_train)

X_flat_train_scaled = scaler_X.transform(X_flat_train)
X_flat_test_scaled = scaler_X.transform(X_seq_test.reshape(-1, num_features))

X_seq_train_scaled = torch.tensor(X_flat_train_scaled.reshape(X_seq_train.shape), dtype=torch.float32)
X_seq_test_scaled = torch.tensor(X_flat_test_scaled.reshape(X_seq_test.shape), dtype=torch.float32)

# Scale y and create dataloaders
scaler_y = StandardScaler().fit(y_seq_train.reshape(-1, 1))
y_seq_train_scaled = torch.tensor(scaler_y.transform(y_seq_train.reshape(-1, 1)), dtype=torch.float32)
y_seq_test_scaled = torch.tensor(scaler_y.transform(y_seq_test.reshape(-1, 1)), dtype=torch.float32)

train_dataset = TensorDataset(X_seq_train_scaled, y_seq_train_scaled)
test_dataset = TensorDataset(X_seq_test_scaled, y_seq_test_scaled)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)

class RNN(nn.Module):
  def __init__(self,input_dim,hidden_dim):
    super(RNN,self).__init__()
    self.hidden_dim=hidden_dim
    self.input_dim=input_dim

    #Define the weights:
    self.Wxh=nn.Linear(input_dim,hidden_dim)
    self.Whh=nn.Linear(hidden_dim,hidden_dim, bias=False)
    self.tanh=nn.Tanh()

    #output
    self.output=nn.Linear(hidden_dim,1)

  def forward(self,x):
    batch_size, seq_len, _= x.shape # Added seq_len dimension
    h=torch.zeros(batch_size,self.Wxh.out_features, device=x.device) # Initialize hidden state on the same device as input

    for t in range(seq_len):
      x_t=x[:, t, :]
      h= self.tanh(self.Wxh(x_t)+self.Whh(h))

    output=self.output(h)
    return output

rnn=RNN(input_dim=18,hidden_dim=32)
loss_fn=nn.MSELoss()
optimizer=torch.optim.Adam(rnn.parameters(),lr=0.001)

#training loop
num_epochs=2000
losses=[]

for epoch in range(num_epochs):
  total_loss=0
  for xb,yb in train_loader:
    pred = rnn(xb) # Use the RNN model
    loss=loss_fn(pred,yb)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    total_loss+=loss.item()*xb.size(0)

  losses.append(total_loss/len(train_loader.dataset))
  if (epoch + 1) % 10 == 0:
    print(f"RNN Epoch {epoch+1}, Loss: {total_loss:.4f}")

#now the test phase
def test_model(model, test_loader, loss_fn,scaler_y):
  model.eval()
  y_true_list=[]
  y_pred_list=[]

  with torch.no_grad():
    total_loss=0
    for xb,yb in test_loader:
      pred=model(xb)
      y_pred_list.append(pred.numpy())
      y_true_list.append(yb.numpy())

  y_pred = np.concatenate(y_pred_list, axis=0)
  y_true = np.concatenate(y_true_list, axis=0)

  y_pred_rescaled = scaler_y.inverse_transform(y_pred)
  y_true_rescaled = scaler_y.inverse_transform(y_true)

  mse = mean_squared_error(y_true_rescaled, y_pred_rescaled)
  mae = mean_absolute_error(y_true_rescaled, y_pred_rescaled)
  r2 = r2_score(y_true_rescaled, y_pred_rescaled)

  print(f"Test MSE: {mse:.2f}")
  print(f"Test MAE: {mae:.2f}")
  print(f"R² Score: {r2:.4f}")
  plt.figure(figsize=(12, 5))
  plt.plot(y_true_rescaled, label="Real values", color="blue")
  plt.plot(y_pred_rescaled, label="Predicted Values", color="orange")
  plt.title("Predicted vs real values curve")
  plt.xlabel("Sample")
  plt.ylabel("UPDRS")
  plt.legend()
  plt.grid(True)
  plt.tight_layout()
  plt.show()

  #Scatter
  plt.figure(figsize=(6, 6))
  plt.scatter(y_true_rescaled, y_pred_rescaled, alpha=0.6, color="royalblue", edgecolors='k')
  plt.plot([y_true_rescaled.min(), y_true_rescaled.max()],
         [y_true_rescaled.min(), y_true_rescaled.max()],
         color='red', linestyle='--', label="Linea ideale")

  plt.title("Scatter plot: Real vs Predicted values")
  plt.xlabel("Real values (UPDRS)")
  plt.ylabel("Predicted values")
  plt.legend()
  plt.grid(True)
  plt.tight_layout()
  plt.show()

test_model(model, test_loader,loss_fn, scaler_y)

"""3# LSTM model"""

#First data processing and organizzation

df_grouped=df_full.groupby('subject#')

#sliding window parameters
window_size=5
target_col="motor_UPDRS"


X_seq=[]
y_seq=[]

#preprocessing to convert data in to sequences is the same as for the RNN
for subject_id, df_subject in df_grouped:
  group_sorted=df_subject.sort_values("test_time").reset_index(drop=True)

  features=group_sorted.drop(columns=['subject#','test_time','motor_UPDRS','total_UPDRS'])
  target=group_sorted[target_col]

  for i in range(len(features)-window_size):
    X_seq.append(features[i:i+window_size])
    y_seq.append(target[i+window_size])

X_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)

X_seq_train=np.array(X_seq)
y_seq_train=np.array(y_seq)
X_seq_test=np.array(X_seq)
y_seq_test=np.array(y_seq)

# Normalizza X
num_features = X_seq_train.shape[2]
X_flat_train = X_seq_train.reshape(-1, num_features)
scaler_X = StandardScaler().fit(X_flat_train)

X_flat_train_scaled = scaler_X.transform(X_flat_train)
X_flat_test_scaled = scaler_X.transform(X_seq_test.reshape(-1, num_features))

X_seq_train_scaled = torch.tensor(X_flat_train_scaled.reshape(X_seq_train.shape), dtype=torch.float32)
X_seq_test_scaled = torch.tensor(X_flat_test_scaled.reshape(X_seq_test.shape), dtype=torch.float32)

# Normalizza y
scaler_y = StandardScaler().fit(y_seq_train.reshape(-1, 1))
y_seq_train_scaled = torch.tensor(scaler_y.transform(y_seq_train.reshape(-1, 1)), dtype=torch.float32)
y_seq_test_scaled = torch.tensor(scaler_y.transform(y_seq_test.reshape(-1, 1)), dtype=torch.float32)

train_dataset = TensorDataset(X_seq_train_scaled, y_seq_train_scaled)
test_dataset = TensorDataset(X_seq_test_scaled, y_seq_test_scaled)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)

"""Then we define the model:"""

class LSTM(nn.Module):
  def __init__(self,input_dim,hidden_dim):
    super(LSTM,self).__init__()
    self.hidden_dim=hidden_dim
    self.input_dim=input_dim

    self.Wf=nn.Linear(input_dim+hidden_dim,hidden_dim)#forget
    self.Wi=nn.Linear(input_dim+hidden_dim,hidden_dim)#input
    self.Wc=nn.Linear(input_dim+hidden_dim,hidden_dim)#cell
    self.Wo=nn.Linear(input_dim+hidden_dim,hidden_dim)#output

    self.tanh=nn.Tanh()
    self.sigmoid=nn.Sigmoid()
    #output of the model
    self.output=nn.Linear(hidden_dim,1)

  def forward(self,x,initial_hidden,initial_cell):
    batch_size, seq_len, _= x.shape
    hidden=initial_hidden
    cell=initial_cell

    for t in range(seq_len):
      x_t=x[:, t, :]
      concat=torch.cat((x_t,hidden),dim=1)
      forget=self.sigmoid(self.Wf(concat))
      input=self.sigmoid(self.Wi(concat))
      candidate=self.tanh(self.Wc(concat))

      output_gate=self.sigmoid(self.Wo(concat))
      cell=forget*cell+input*candidate
      hidden=output_gate*self.tanh(cell)

    output=self.output(hidden)

    return output,hidden,cell


ltsm=LSTM(input_dim=18,hidden_dim=32)
loss_fn=nn.MSELoss()
optimizer=torch.optim.Adam(ltsm.parameters(),lr=0.001) # Optimizer for the model

"""Now training the model:"""

#training loop
num_epochs=2000
losses=[]

for epoch in range(num_epochs):
  total_loss=0
  for xb,yb in train_loader:
    hidden=torch.zeros(xb.size(0), ltsm.hidden_dim, device=xb.device)
    cell=torch.zeros(xb.size(0), ltsm.hidden_dim, device=xb.device)
    pred,hidden,cell = ltsm(xb,hidden,cell) # Use the lstm
    loss=loss_fn(pred,yb)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    total_loss+=loss.item()*xb.size(0)

  losses.append(total_loss/len(train_loader.dataset))
  if (epoch + 1) % 10 == 0:
    print(f"RNN Epoch {epoch+1}, Loss: {total_loss:.4f}")

#now the test phase
def test_model(model, test_loader, loss_fn,scaler_y):
  model.eval()
  y_true_list=[]
  y_pred_list=[]

  with torch.no_grad():
    total_loss=0
    for xb,yb in test_loader:
      pred,_,_=model(xb,torch.zeros(xb.size(0), model.hidden_dim, device=xb.device), torch.zeros(xb.size(0), model.hidden_dim, device=xb.device))
      y_pred_list.append(pred.numpy())
      y_true_list.append(yb.numpy())

  y_pred = np.concatenate(y_pred_list, axis=0)
  y_true = np.concatenate(y_true_list, axis=0)

  y_pred_rescaled = scaler_y.inverse_transform(y_pred)
  y_true_rescaled = scaler_y.inverse_transform(y_true)

  mse = mean_squared_error(y_true_rescaled, y_pred_rescaled)
  mae = mean_absolute_error(y_true_rescaled, y_pred_rescaled)
  r2 = r2_score(y_true_rescaled, y_pred_rescaled)

  print(f"Test MSE: {mse:.2f}")
  print(f"Test MAE: {mae:.2f}")
  print(f"R² Score: {r2:.4f}")
  plt.figure(figsize=(12, 5))
  plt.plot(y_true_rescaled, label="Real values", color="blue")
  plt.plot(y_pred_rescaled, label="Predicted values", color="orange")
  plt.title("Predicted vs real values curve")
  plt.xlabel("Sample")
  plt.ylabel("UPDRS")
  plt.legend()
  plt.grid(True)
  plt.tight_layout()
  plt.show()

  #Scatter
  plt.figure(figsize=(6, 6))
  plt.scatter(y_true_rescaled, y_pred_rescaled, alpha=0.6, color="royalblue", edgecolors='k')
  plt.plot([y_true_rescaled.min(), y_true_rescaled.max()],
         [y_true_rescaled.min(), y_true_rescaled.max()],
         color='red', linestyle='--', label="Linea ideale")

  plt.title("Scatter plot: Real vs Predicted values")
  plt.xlabel("Real values (UPDRS)")
  plt.ylabel("Predicted values")
  plt.legend()
  plt.grid(True)
  plt.tight_layout()
  plt.show()

test_model(ltsm, test_loader,loss_fn, scaler_y)

"""now we implement the attention mechanism in the lstm architecture."""

#first define and train a similar lstm model, with one small difference
class LSTM_for_attention(nn.Module):
  def __init__(self,input_dim,hidden_dim):
    super(LSTM_for_attention,self).__init__()
    self.hidden_dim=hidden_dim
    self.input_dim=input_dim

    self.Wf=nn.Linear(input_dim+hidden_dim,hidden_dim) #forget
    self.Wi=nn.Linear(input_dim+hidden_dim,hidden_dim)#input
    self.Wc=nn.Linear(input_dim+hidden_dim,hidden_dim)#cell
    self.Wo=nn.Linear(input_dim+hidden_dim,hidden_dim)#output

    self.tanh=nn.Tanh()
    self.sigmoid=nn.Sigmoid()
    #output
    self.output=nn.Linear(hidden_dim,1)

  def forward(self,x,initial_hidden,initial_cell):
    batch_size, seq_len, _= x.shape
    hidden=initial_hidden
    cell=initial_cell
    hidden_seq=[] #we need a sequence accumulating all the hidden states we compute
    for t in range(seq_len):
      x_t=x[:, t, :]
      concat=torch.cat((x_t,hidden),dim=1)
      forget=self.sigmoid(self.Wf(concat))
      input=self.sigmoid(self.Wi(concat))
      candidate=self.tanh(self.Wc(concat))

      output_gate=self.sigmoid(self.Wo(concat))
      cell=forget*cell+input*candidate
      hidden=output_gate*self.tanh(cell)
      hidden_seq.append(hidden.unsqueeze(1))  #shape [batch,1,hidden_dim]

    hidden_seq=torch.cat(hidden_seq,dim=1) #shape [batch,seq_len,hidden_dim]
    #output=self.output(hidden)   NO NEED FOR THIS OUTPUT

    return hidden_seq,hidden,cell


class AttentionLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super.__init__()
        self.hidden_dim = hidden_dim
        self.lstm=LSTM_for_attention(input_dim=18,hidden_dim=32)
        self.attention = nn.Linear(hidden_dim, 1)
        self.output = nn.Linear(hidden_dim, 1)

    def forward(self, x, initial_hidden, initial_cell):
        #x: [batch_size,sew_len,input_dim]
        lstm_out=self.lstm(x,initial_hidden,initial_cell)

        #attention weights
        attn_weights=torch.softmax(self.attention(lstm_out),dim=1)

        #context vector
        context=torch.sum(attn_weights*lstm_out,dim=1)

        #output
        output=self.output(context)
        return output

A_lstm=LSTM(input_dim=18,hidden_dim=32)
loss_fn=nn.MSELoss()
optimizer=torch.optim.Adam(A_lstm.parameters(),lr=0.001) # Optimizer for the model

"""Qui il ciclo di training:"""

#training loop
num_epochs=2000
losses=[]

for epoch in range(num_epochs):
  total_loss=0
  for xb,yb in train_loader:
    hidden=torch.zeros(xb.size(0), ltsm.hidden_dim, device=xb.device)
    cell=torch.zeros(xb.size(0), ltsm.hidden_dim, device=xb.device)
    pred,hidden,cell = A_lstm(xb,hidden,cell) # Use the model
    loss=loss_fn(pred,yb)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step() # Call optimizer.step() as a function
    total_loss+=loss.item()*xb.size(0)

  losses.append(total_loss/len(train_loader.dataset))
  if (epoch + 1) % 10 == 0:
    print(f"RNN Epoch {epoch+1}, Loss: {total_loss:.4f}")

#we use the same testing function as the previous model
test_model(A_lstm, test_loader,loss_fn, scaler_y)

"""The last model is a bidirectional lstm, we already have the lstm that reads normally (left to right), now we implement the other one that reads in reverse"""

class LSTM_bw(nn.Module):
  def __init__(self,input_dim,hidden_dim):
    super(LSTM_bw,self).__init__()
    self.hidden_dim=hidden_dim
    self.input_dim=input_dim
    self.lstm_fw=LSTM(input_dim=18,hidden_dim=32)
    self.lstm_bw=LSTM(input_dim=18,hidden_dim=32)

    self.tanh=nn.Tanh()
    self.sigmoid=nn.Sigmoid()
    #output
    self.output=nn.Linear(hidden_dim*2,1) # Output dimension is hidden_dim*2

  def forward(self,x):
    batch_size, seq_len, _= x.shape
    h0 = torch.zeros(x.size(0), self.lstm_fw.hidden_dim, device=x.device)
    c0 = torch.zeros(x.size(0), self.lstm_fw.hidden_dim, device=x.device)

    _,hidden_fwd,_=self.lstm_fw(x,h0,c0) # Get the final hidden state
    _,hidden_bw,_=self.lstm_bw(torch.flip(x,[1]),h0,c0) # Get the final hidden state

    combined_hidden=torch.cat((hidden_fwd,hidden_bw),dim=1) # Concatenate along dimension 1

    return self.output(combined_hidden)

bi_lstm=LSTM_bw(input_dim=18,hidden_dim=32)
loss_fn=nn.MSELoss()
optimizer=torch.optim.Adam(bi_lstm.parameters(),lr=0.001) # Optimizer for the model
num_epochs=2000
losses=[]
#Training loop
for epoch in range(num_epochs):
  total_loss=0
  for xb,yb in train_loader:
    pred = bi_lstm(xb) # Use the model, it returns only the prediction
    loss=loss_fn(pred,yb)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step() # Call optimizer.step() as a function
    total_loss+=loss.item()*xb.size(0)

  losses.append(total_loss/len(train_loader.dataset))
  if (epoch + 1) % 10 == 0:
    print(f"RNN Epoch {epoch+1}, Loss: {total_loss:.4f}")

#Now we test the model:
def test_model(model, test_loader, loss_fn,scaler_y):
  model.eval()
  y_true_list=[]
  y_pred_list=[]

  with torch.no_grad():
    total_loss=0
    for xb,yb in test_loader:
      pred=model(xb)
      y_pred_list.append(pred.numpy())
      y_true_list.append(yb.numpy())

  y_pred = np.concatenate(y_pred_list, axis=0)
  y_true = np.concatenate(y_true_list, axis=0)

  y_pred_rescaled = scaler_y.inverse_transform(y_pred)
  y_true_rescaled = scaler_y.inverse_transform(y_true)

  mse = mean_squared_error(y_true_rescaled, y_pred_rescaled)
  mae = mean_absolute_error(y_true_rescaled, y_pred_rescaled)
  r2 = r2_score(y_true_rescaled, y_pred_rescaled)

  print(f"Test MSE: {mse:.2f}")
  print(f"Test MAE: {mae:.2f}")
  print(f"R² Score: {r2:.4f}")
  plt.figure(figsize=(12, 5))
  plt.plot(y_true_rescaled, label="Valori reali", color="blue")
  plt.plot(y_pred_rescaled, label="Valori predetti", color="orange")
  plt.title("Predicted vs real values curve")
  plt.xlabel("Sample")
  plt.ylabel("UPDRS")
  plt.legend()
  plt.grid(True)
  plt.tight_layout()
  plt.show()

  #Scatter
  plt.figure(figsize=(6, 6))
  plt.scatter(y_true_rescaled, y_pred_rescaled, alpha=0.6, color="royalblue", edgecolors='k')
  plt.plot([y_true_rescaled.min(), y_true_rescaled.max()],
         [y_true_rescaled.min(), y_true_rescaled.max()],
         color='red', linestyle='--', label="Linea ideale")

  plt.title("Scatter plot: Real vs Predicted values")
  plt.xlabel("Real values (UPDRS)")
  plt.ylabel("Predicted values")
  plt.legend()
  plt.grid(True)
  plt.tight_layout()
  plt.show()

test_model(bi_lstm, test_loader,loss_fn, scaler_y)